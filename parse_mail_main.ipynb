{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,pg8000\n",
    "from numpy.lib.utils import source\n",
    "import unicodedata\n",
    "import tarfile\n",
    "import gzip\n",
    "import email, codecs\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "#import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config.database import HOST, PORT, USER, PASSWORD, DATABASE\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "os.chdir(\"/mnt/data0/proj_osgeo/data\")\n",
    "db_config=open('./db_config')\n",
    "dbHOST=db_config.readline().split('\\\"')[1]\n",
    "dbUSER=db_config.readline().split('\\\"')[1]\n",
    "dbPASS=db_config.readline().split('\\\"')[1]\n",
    "dbDB=db_config.readline().split('\\\"')[1]\n",
    "db_config.close()\n",
    "conn = pg8000.connect(host=dbHOST,user=dbUSER,password=dbPASS,database=dbDB)\n",
    "conn.commit()\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import types as sqltype\n",
    "from config.database import HOST, PORT, USER, PASSWORD, DATABASE\n",
    "\n",
    "psql_engine = create_engine(\"postgresql://\"+USER+\":\"+PASSWORD+\"@\"+HOST+\":\"+str(PORT)+\"/\"+DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def un_gz(file_name):\n",
    "    f_name = file_name.replace(\".gz\",\"\")\n",
    "    g_file = gzip.GzipFile(file_name)\n",
    "    open(f_name, \"wb+\").write(g_file.read())\n",
    "    g_file.close()\n",
    "\n",
    "    return(f_name)\n",
    "\n",
    "\n",
    "\n",
    "def escape(s):\n",
    "    return (s).replace(\"'\", \"''\").replace('%', '%%').replace('<','').replace('>','')\n",
    "\n",
    "\n",
    "\n",
    "def read_into_buffer(filename):\n",
    "    buf = bytearray(os.path.getsize(filename))\n",
    "    with open(filename, 'rb') as f:\n",
    "        f.readinto(buf)\n",
    "    f.close()\n",
    "    return buf\n",
    "\n",
    "def readfileline(f):\n",
    "    \n",
    "    try:\n",
    "        return f.readline()\n",
    "    except Exception as e:\n",
    "        #print(f.tell())\n",
    "        f.seek(f.tell()+5, 0)\n",
    "        #print(f.tell())\n",
    "        #print(\"Jump 5\")\n",
    "        return -1\n",
    "    \n",
    "def readfile(path, enco):\n",
    "    try:\n",
    "        with open(path, 'r', encoding= enco) as f:\n",
    "            return f.readlines()\n",
    "    except BaseException as err:\n",
    "        try:\n",
    "            x = []\n",
    "            with open(path, 'r', encoding= enco) as f:\n",
    "                fline = readfileline(f)\n",
    "                while fline != \"\":\n",
    "                    if fline != -1 :\n",
    "                        x.append(fline)\n",
    "                    fline = readfileline(f)\n",
    "                    #print(fline)\n",
    "                return x\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "\n",
    "def _decode(b, enco):\n",
    "    try:\n",
    "        return str(b, encoding= enco)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def decode_data(b, added_encode=None):\n",
    "    \"\"\"\n",
    "    bytedecoding\n",
    "    :param bytes:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #def _decode(b, enco):\n",
    "    #    try:\n",
    "    #        return str(b, encoding= enco)\n",
    "    #    except Exception as e:\n",
    "    #        return None\n",
    "\n",
    "    encodes = ['utf-8', 'ascii', \"base64\"]\n",
    "    if added_encode:\n",
    "        encodes = [added_encode] + encodes\n",
    "    for enco in encodes:\n",
    "        str_data = _decode(b, enco)\n",
    "        if str_data != None:\n",
    "            return str_data\n",
    "    return None\n",
    "   \n",
    " \n",
    "# Converts text into ascii to fix weird characters\n",
    "def asciiCodify(s):\n",
    "    try:\n",
    "        u = unicode(s, errors=\"replace\")\n",
    "        v = u.encode(\"ascii\", \"replace\")\n",
    "        return v\n",
    "    except:\n",
    "        # s is empty\n",
    "        return \"EMPTY\"\n",
    "# Fix any characters that might mess up script\n",
    "def escape(s):\n",
    "    return (s).replace(\"'\", \"''\").replace('%', '%%').replace('<','').replace('>','')\n",
    "    \n",
    "# Counts number of lines in file\n",
    "def file_line_number(file):\n",
    "    for i, l in enumerate(file):\n",
    "        pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def time_filename(file_csv_route):\n",
    "    try:\n",
    "        file_month = file_csv_route.split('/')[-1].split('.')[0]\n",
    "        #time_format = \"%Y-%B\"\n",
    "        #str_time = file_month.split('.')[0]\n",
    "        #time_struct = time.strptime(str_time, time_format)\n",
    "        #timeStamp = pd.Timestamp(time_struct)\n",
    "\n",
    "        timeStamp = pd.Timestamp(file_month)    #pandas powerful!\n",
    "        return timeStamp\n",
    "    except BaseException as err:\n",
    "        print(err)\n",
    "        return 0\n",
    "\n",
    "def time_parse(message_date):\n",
    "    \n",
    "    try:\n",
    "        time_format = \"%a %b %d %H:%M:%S %Y\"\n",
    "        tsp = time.strptime(message_date, time_format)\n",
    "    except ValueError as e:\n",
    "        try:\n",
    "            time_format =  \"%a %d %b %Y %H:%M:%S %z \"\n",
    "            tsp = time.strptime(message_date.split('(')[0].replace(\",\",\"\"), time_format)\n",
    "        except ValueError as e:\n",
    "            try:\n",
    "                time_format =  \"%a %d %b %Y %H:%M:%S %z\"\n",
    "                tsp = time.strptime(message_date.split('(')[0].replace(\",\",\"\"), time_format)\n",
    "            except ValueError as e:\n",
    "                try:\n",
    "                    time_format =  \"%d %b %Y %H:%M:%S %z\"\n",
    "                    tsp = time.strptime(message_date.split('(')[0].replace(\",\",\"\"), time_format)\n",
    "                except ValueError as e:\n",
    "                    try:\n",
    "                        time_format = \"%a %d %b %Y %H:%M:%S\"\n",
    "                        tsp = time.strptime(message_date.split('(')[0][:-5].replace(\",\",\"\"), time_format)\n",
    "                    except ValueError as e:\n",
    "                        try:\n",
    "                            time_format = \"%a %d %b %Y %H:%M \"\n",
    "                            tsp = time.strptime(message_date.split('(')[0][:-5].replace(\",\",\"\"), time_format)\n",
    "                        except ValueError as e:\n",
    "                            try:\n",
    "                                time_format = \"%a %b %d %H:%M:%S %Y\"\n",
    "                                tsp = time.strptime(message_date.split('(')[0][:-5].replace(\",\",\"\"), time_format)\n",
    "                            except BaseException as err:\n",
    "                                    print(err, \"Return timestamp as 0\")\n",
    "                                    #flag = True\n",
    "                                    return 0\n",
    "    time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    return(pd.Timestamp(time.strftime(time_format, tsp)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "def getMonthContent(path):\n",
    "    #print(path)\n",
    "    messages = []\n",
    "    messageText=\"\"\n",
    "    try:\n",
    "        #fileContentIn = codecs.open(path)\n",
    "        #global ContentIn\n",
    "        #ContentIn=[decode_data(x) for x in fileContentIn.readlines()]\n",
    "        #ContentIn = fileContentIn.readlines()\n",
    "\n",
    "\n",
    "        encodes = ['utf-8', 'ascii', \"base64\", 'ANSI', 'GBK', 'utf-16', 'utf-32']\n",
    "        for enco in encodes:\n",
    "            ContentIn = readfile(path, enco)\n",
    "            if ContentIn is not None:\n",
    "                #print(\"File read Success:\"+path)\n",
    "                break\n",
    "        \n",
    "        #print(ContentIn)\n",
    "        #fileContentIn.close()\n",
    "        if ContentIn is None:\n",
    "            raise ValueError(\"Unknown File Coding of file:\" + path)\n",
    "        #messages=[]\n",
    "        \n",
    "        for i in range(len(ContentIn)):\n",
    "            if (i+1 < len(ContentIn) and ContentIn[i].startswith(\"From \") and ContentIn[i+1].startswith(\"From:\")) or (i+1==len(ContentIn)):\n",
    "                if len(messageText):\n",
    "                    message = email.message_from_string(messageText)\n",
    "                    #print( \"Message\",\"From\", message[\"from\"], \"To\", message[\"to\"], \"ID\", message[\"message-id\"], \"Thread\", message[\"references\"])\n",
    "                    messages.append(message)\n",
    "                messageText = \"\"\n",
    "            else:\n",
    "                messageText += ContentIn[i]\n",
    "        #print (\"there are\", len(messages), \" messages\",\" \",path)\n",
    "        #global totalMessages\n",
    "        #totalMessages = totalMessages + len(messages)\n",
    "    except BaseException as e:\n",
    "        print(e)\n",
    "        \n",
    "    if len(messages) == 0:\n",
    "        return [\"\"]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def add_aliase(aliase_id, personname, mailaddress, source):\n",
    "\n",
    "    try:\n",
    "        db = psycopg2.connect(\n",
    "            host=HOST,\n",
    "            port=PORT,\n",
    "            user=USER,\n",
    "            password=PASSWORD,\n",
    "            database=DATABASE)\n",
    "            #charset='utf8')\n",
    "        cursor = db.cursor()\n",
    "    except Exception as e:\n",
    "        print(\"Database connect error:%s\" % e)\n",
    "\n",
    "    #aliase_id, mailaddress = aliase.replace('<', ' ').replace('>', ' ').split()\n",
    "    aliase_id = escape(aliase_id)\n",
    "    sql_aliase = \"\"\"INSERT INTO aliase(aliase_id, personname, mailaddress, source)\n",
    "                                        values('%s', '%s', '%s', '%s')\"\"\" % (aliase_id, personname, mailaddress, source)\n",
    "    try:\n",
    "        db.commit()\n",
    "        cursor.execute(sql_aliase)\n",
    "        db.commit()\n",
    "    except Exception as err:\n",
    "        sql_aliase = \"\"\"UPDATE aliase SET personname='%s', mailaddress='%s', source='%s' WHERE aliase_id='%s' \"\"\" % (personname, mailaddress, source, aliase_id)\n",
    "        \n",
    "        try:\n",
    "            db.commit()\n",
    "            cursor.execute(sql_aliase)\n",
    "            db.commit()\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "\n",
    "    db.close()\n",
    "    return aliase_id\n",
    "\n",
    "def add_thread(messages, pjname):\n",
    "    try:\n",
    "        db = psycopg2.connect(\n",
    "            host=HOST,\n",
    "            port=PORT,\n",
    "            user=USER,\n",
    "            password=PASSWORD,\n",
    "            database=DATABASE)\n",
    "            #charset='utf8')\n",
    "        cursor = db.cursor()\n",
    "    except Exception as e:\n",
    "        print(\"Database connect error:%s\" % e)\n",
    "        return\n",
    "    \n",
    "\n",
    "    project_id = escape(pjname)\n",
    "    \n",
    "    #OLD PART BELOW#\n",
    "    #    global counter_thread\n",
    "    #    global counter_message\n",
    "    #    for message in messages:\n",
    "    #        counter_message+= 1\n",
    "    #        if \"references\" in message:\n",
    "    #            \n",
    "    #            thread_id_old = escape(message[\"references\"].split()[-1]+\"__\"+pjname)\n",
    "    #            \n",
    "    #            thread_id = escape(project_id+\"#\"+str(counter_thread)+\"#\"+message[\"references\"].split()[-1])\n",
    "    #\n",
    "    #            sql_remove = \"\"\"DELETE FROM thread WHERE thread_id='%s' \"\"\" % (thread_id_old)\n",
    "    #            try:\n",
    "    #                db.commit()\n",
    "    #                cursor.execute(sql_remove)\n",
    "    #                db.commit()\n",
    "    #            except BaseException as err:\n",
    "    #                pass\n",
    "    #            \n",
    "    #        elif \"message-id\" in message:\n",
    "    #            counter_thread+=1\n",
    "    #            project_id = escape(pjname)\n",
    "    #            thread_id = escape(project_id+\"#\"+str(counter_thread)+\"#\"+message[\"message-id\"].split()[-1])\n",
    "    #        else:\n",
    "    #            counter_thread+= 1\n",
    "    #            thread_id= escape(pjname+\"#\"+str(counter_thread)+\"#\")\n",
    "    #            continue\n",
    "    #OLD PART ABOVE#\n",
    "    threads = messages[\"thread_id\"].drop_duplicates().values\n",
    "    for thread_id in threads:\n",
    "        \n",
    "        thread_id = str(thread_id)\n",
    "        thread_type = \"emails\"\n",
    "        thread_name = escape(project_id+thread_id.split('@')[0])\n",
    "        #aliase_id, mailaddress = aliase.replace('<', ' ').replace('>', ' ').split()\n",
    "        #thread_id = escape(thread_id)\n",
    "        \n",
    "        \n",
    "\n",
    "        sql_thread = \"\"\"INSERT INTO thread(thread_id, thread_name, project_id, thread_type)\n",
    "                                            values('%s', '%s', '%s', '%s')\"\"\" % (thread_id, thread_name, project_id, thread_type)\n",
    "\n",
    "        try:\n",
    "            db.commit()\n",
    "            cursor.execute(sql_thread)\n",
    "            db.commit()\n",
    "        except Exception as err:\n",
    "            sql_thread = \"\"\"UPDATE thread SET thread_name='%s', project_id='%s', thread_type='%s' WHERE thread_id='%s' \"\"\" \\\n",
    "                            % (thread_name, project_id, thread_type, thread_id)\n",
    "            #print(err)\n",
    "        \n",
    "            try:\n",
    "                db.commit()\n",
    "                cursor.execute(sql_thread)\n",
    "                db.commit()\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "\n",
    "    db.close()\n",
    "    return 0\n",
    "\n",
    "def checkAliase(conn, messages):\n",
    "    \n",
    "    list_person = []\n",
    "    #list_mailaddress = []\n",
    "    #list_id = []\n",
    "\n",
    "    for message in messages:\n",
    "        if \"from\" in message:\n",
    "            personname = email.utils.parseaddr(message[\"from\"])[0]\n",
    "            mailaddress = message[\"from\"].split('(')[0].replace(' at ','@')\n",
    "            # NEED REDO with #!!!!!!!!!!!!!!\n",
    "            list_person.append([escape(personname+'_'+mailaddress), escape(personname), escape(mailaddress)])\n",
    "            #list_mailaddress.append(escape(mailaddress))\n",
    "            #list_id.append(escape(personname+'_'+mailaddress))\n",
    "\n",
    "    df_aliases = pd.DataFrame(list_person).drop_duplicates()\n",
    "\n",
    "    #add_aliase(escape(personname+'_'+mailaddress), escape(personname), escape(mailaddress), \"email\")\n",
    "    for id, name, mailadd in df_aliases.values:\n",
    "        add_aliase(id, name, mailadd, \"emails\") \n",
    "            \n",
    "\n",
    "def checkThread(conn, messages, pjname):\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    threads = set()\n",
    "    cursor.execute(\"select thread_id, project_id from thread\")\n",
    "    result=cursor.fetchall()\n",
    "    for thread_id, project_id in result:\n",
    "        threads.add((thread_id, project_id))\n",
    "        \n",
    "    unknownThreads = set()\n",
    "    for message in messages:\n",
    "        if \"references\" in message:\n",
    "            thread_id = escape(message[\"references\"].split()[-1]+\"__\"+pjname)\n",
    "            project_id = escape(pjname)\n",
    "            if (thread_id, project_id) not in threads:\n",
    "                if (thread_id, project_id) not in unknownThreads:\n",
    "                    unknownThreads.add((thread_id, project_id))\n",
    "        #else:\n",
    "            #print(\"No threading\")\n",
    "    \n",
    "    if len(unknownThreads) > 0:\n",
    "        #print(unknownThreads)\n",
    "        sql = \"insert into thread (thread_id, thread_name, project_id) values \" + \\\n",
    "                \",\".join([\"('\"+ thread_id+\"','\" + project_id+thread_id.split('@')[0]+\"','\" + project_id + \"')\"\n",
    "                            for thread_id, project_id in unknownThreads]) + \\\n",
    "                \" returning thread_id, project_id\"\n",
    "        \n",
    "        try:\n",
    "            cursor.execute(sql)\n",
    "            result=cursor.fetchall()\n",
    "            for thread_id, project_id in result:\n",
    "                threads.add((thread_id, project_id))\n",
    "        except pg8000.ProgrammingError as e:\n",
    "            #print message\n",
    "            print (\"insert error\")\n",
    "            print (e)\n",
    "            \n",
    "        conn.commit()\n",
    "\n",
    "def saveMessagetocsv(messages, csv_file, element):\n",
    "    \n",
    "    message_id = []\n",
    "    thread_id = []\n",
    "    author_aliase_id = []\n",
    "    author_name = []\n",
    "    message_text = []\n",
    "    timestamp = []\n",
    "    subject = []\n",
    "    reply_to = []\n",
    "    references = []\n",
    "    global counter_thread\n",
    "    global dict_thread\n",
    "    global counter_message\n",
    "    for message in messages:\n",
    "        #flag = False\n",
    "        counter_message+= 1\n",
    "        if \"from\" in message:\n",
    "            personname = email.utils.parseaddr(message[\"from\"])[0]\n",
    "            #personname = asciiCodify(personname)  #不可用\n",
    "            mailaddress = message[\"from\"].split('(')[0].replace(' at ','@')\n",
    "\n",
    "            author_aliase_id.append(escape(personname+'_'+mailaddress))\n",
    "            author_name.append(escape(personname))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if \"message-id\" in message:\n",
    "                message_id.append(escape(element+\"#\"+str(counter_message)+\"#\"+message[\"message-id\"]))\n",
    "            else:\n",
    "                message_id.append(escape(element+\"#\"+str(counter_message)+\"#\"))\n",
    "        except BaseException as e:\n",
    "            print(e)\n",
    "            message_id.append(escape(element+\"#\"+str(counter_message)+\"#\"))\n",
    "        #print(message_id[-1])\n",
    "\n",
    "        if \"references\" in message:\n",
    "            references.append(message[\"references\"].split())\n",
    "            reference = message[\"references\"].split()[0] ###########\n",
    "            if reference in dict_thread:\n",
    "                thread_no = dict_thread[reference]\n",
    "                thread_id.append(escape(element+\"#\"+str(thread_no)+\"#\"+message[\"references\"].split()[0]))\n",
    "            else:\n",
    "                counter_thread+= 1\n",
    "                thread_no = counter_thread\n",
    "                dict_thread[reference] = thread_no\n",
    "                thread_id.append(escape(element+\"#\"+str(thread_no)+\"#\"+message[\"references\"].split()[0]))\n",
    "        elif \"message-id\" in message:\n",
    "            references.append(np.nan)\n",
    "            reference = message[\"message-id\"]\n",
    "            counter_thread+= 1\n",
    "            thread_no = counter_thread\n",
    "            dict_thread[reference] = thread_no\n",
    "            thread_id.append(escape(element+\"#\"+str(thread_no)+\"#\"+message[\"message-id\"]))\n",
    "        else:\n",
    "            references.append(np.nan)\n",
    "            counter_thread+= 1\n",
    "            thread_no = counter_thread\n",
    "            dict_thread[reference] = thread_no\n",
    "            thread_id.append(escape(element+\"#\"+str(thread_no)+\"#\"))\n",
    "        \n",
    "\n",
    "        message_text.append(escape(message.__str__()))\n",
    "\n",
    "        if \"subject\" in message:\n",
    "            subject.append(escape(message[\"subject\"]))\n",
    "        else:\n",
    "            subject.append(\"\")\n",
    "\n",
    "        if 'In-Reply-To' in message:\n",
    "            reply_to.append(escape(message['In-Reply-To']))\n",
    "        else:\n",
    "            reply_to.append(\"\")\n",
    "\n",
    "        if \"date\" in message:\n",
    "            try:\n",
    "                timestamp.append(pd.Timestamp(message[\"date\"]))\n",
    "            except BaseException as err:\n",
    "                #print(err)\n",
    "                timestamp.append(max(time_filename(csv_file), time_parse(message[\"date\"])))\n",
    "\n",
    "            \n",
    "        else:\n",
    "            timestamp.append(0)\n",
    "            \n",
    "        conn.commit()\n",
    "        #sql = \"insert into message (message_id, thread_id, author_aliase_id, author_name, message_text, timestamp) values \" + \\\n",
    "        #        \",('\"+ message_id[-1]+ \"','\"+ thread_id[-1]+ \"','\"+ author_aliase_id[-1]+ \"','\"+ author_name[-1]+ \"','\"+ '' + \"','\"+ timestamp[-1]+ \"'),\"+ \\\n",
    "        #        \"returning message_id\"\n",
    "        #print(sql)\n",
    "        #try:\n",
    "        #    cursor.execute(sql)\n",
    "            \n",
    "        #except pg8000.ProgrammingError as e:\n",
    "            #print message\n",
    "        #    print (\"insert error\")\n",
    "        #    print (e)\n",
    "        #    print (sql)\n",
    "        conn.commit()\n",
    "        \n",
    "    df_message = pd.DataFrame(columns =[\"message_id\", \"thread_id\", \"author_aliase_id\", \"author_name\", \"receivers_name\", \"message_text\", \"timestamp\" ])\n",
    "    df_message[\"message_id\"] = pd.Series(message_id)\n",
    "    df_message[\"thread_id\"] = pd.Series(thread_id)\n",
    "    df_message[\"author_aliase_id\"] = pd.Series(author_aliase_id)\n",
    "    df_message[\"author_name\"] = pd.Series(author_name)\n",
    "    df_message[\"timestamp\" ] = pd.Series(timestamp)#.apply(lambda x: pd.Timestamp(x))\n",
    "    df_message[\"message_text\" ] = pd.Series(message_text)\n",
    "    df_message[\"subject\"] = pd.Series(subject)\n",
    "    df_message[\"reply_to\"] = pd.Series(reply_to)\n",
    "    df_message[\"mail_references\"] = pd.Series(str(references))\n",
    "    \n",
    "    df_message.to_csv(csv_file)\n",
    "\n",
    "    #print(message.keys())\n",
    "    return df_message\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_psql(data1):\n",
    "    \n",
    "    df_psql = data1.astype(str)\n",
    "    df_psql['timestamp'] = df_psql['timestamp'].apply(lambda x: x.replace('NaT',\"NULL\"))\n",
    "    for col in df_psql.columns.values :\n",
    "        df_psql[col]= df_psql[col].apply(lambda x : x.encode('utf-8','ignore').decode(\"utf-8\").replace(\"\\x00\", \"\\uFFFD\"))\n",
    "        df_psql[col]= df_psql[col].apply(lambda x : \"NULL\" if x=='nan' else x)\n",
    "    df_psql = df_psql.drop(df_psql[df_psql['message_id']==\"NULL\"].index)\n",
    "    df_psql.to_sql(name='message', con=psql_engine, if_exists= 'append', index= False, chunksize=1)\n",
    "\n",
    "    print(\"Data Saved to psql!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Folder: openlayers-dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/54 [00:57<50:26, 57.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Saved to psql!\n",
      "Starting Folder: pywps-dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2/54 [01:05<24:51, 28.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Saved to psql!\n",
      "Starting Folder: openlayers-trac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/54 [01:22<19:44, 23.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Saved to psql!\n",
      "Starting Folder: pycsw-devel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4/54 [01:29<14:01, 16.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Saved to psql!\n",
      "Starting Folder: mapserver-commits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 5/54 [03:06<37:26, 45.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Saved to psql!\n",
      "Starting Folder: postgis-devel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 6/54 [05:30<1:03:13, 79.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Saved to psql!\n",
      "Starting Folder: gdal-dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 7/54 [11:34<2:15:01, 172.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Saved to psql!\n",
      "Starting Folder: grass-commit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 7/54 [14:20<1:36:17, 122.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-15a70e68051f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mdf_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_thismonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mcheckAliase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop_duplicates\u001b[0;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   5110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5111\u001b[0m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inplace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5112\u001b[0;31m         \u001b[0mduplicated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5114\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mduplicated\u001b[0;34m(self, subset, keep)\u001b[0m\n\u001b[1;32m   5249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5250\u001b[0m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5251\u001b[0;31m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5253\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_group_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxnull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(vals)\u001b[0m\n\u001b[1;32m   5224\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5225\u001b[0m             labels, shape = algorithms.factorize(\n\u001b[0;32m-> 5226\u001b[0;31m                 \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SIZE_HINT_LIMIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5227\u001b[0m             )\n\u001b[1;32m   5228\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(values, sort, na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         codes, uniques = _factorize_array(\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_hint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m         )\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36m_factorize_array\u001b[0;34m(values, na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhash_klass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_hint\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     uniques, codes = table.factorize(\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.StringHashTable.factorize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.StringHashTable._unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"Convert the input to an array.\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "global counter_thread\n",
    "global counter_message\n",
    "global dict_thread\n",
    "# Unzip the txt files\n",
    "DIR = './raw'\n",
    "DIR_csv = './csv'\n",
    "filelist_in=os.listdir(DIR)\n",
    "\n",
    "################################Check Project Exist\n",
    "\n",
    "cursor = conn.cursor()\n",
    "conn.commit()\n",
    "\n",
    "projs = set()\n",
    "cursor.execute(\"select proj_id from project\")\n",
    "result=cursor.fetchall()\n",
    "for proj_id in result:\n",
    "    #print(proj_id)\n",
    "    projs.add(proj_id[0])\n",
    "conn.commit()\n",
    "    \n",
    "df_all = pd.DataFrame(columns =[\"message_id\", \"thread_id\", \"author_aliase_id\", \"author_name\", \"receivers_name\", \"message_text\", \"timestamp\", \"subject\", \"reply_to\" ])\n",
    "\n",
    "#for element in filelist_in[11:12]:\n",
    "for element in tqdm(filelist_in):\n",
    "    \n",
    "    print(\"Starting Folder:\", element)\n",
    "    counter_thread = 0\n",
    "    counter_message = 0\n",
    "    dict_thread = {}\n",
    "    element_messages = []\n",
    "    \n",
    "\n",
    "    # Project name and write the log\n",
    "    list_out=open(\"filelist.txt\",\"w+\")\n",
    "    if (\"-commit\" in element) or (\"-dev\" in element)or (\"-psc\" in element)or (\"-discuss\" in element):\n",
    "        pjname = element.split(\"-\")[0]\n",
    "    elif (\"_commit\" in element) or (\"_dev\" in element):\n",
    "        pjname = element.split(\"_\")[0]\n",
    "    else:\n",
    "        pjname = element.split('-')[0]\n",
    "    list_out.write(DIR+\"/\"+element+\"\\n\")\n",
    "    list_out.close()\n",
    "    \n",
    "    if pjname not in projs:\n",
    "        sql = \"insert into project (proj_id) values \" + \"('\"+ escape(pjname) + \"')\" \n",
    "        \n",
    "        cursor.execute(sql)\n",
    "        conn.commit()\n",
    "        projs.add(pjname)\n",
    "        \n",
    "    # UNZIP ALL .gz files, run once\n",
    "    #gzlist = os.listdir(DIR+\"/\"+element)\n",
    "\n",
    "    #for f_gz in gzlist:\n",
    "    #    f_gz_dir = DIR+\"/\"+element+\"/\"+f_gz\n",
    "    #    print(f_gz_dir)\n",
    "    #    if not os.path.exists(DIR+\"/\"+element+\"/\"+\"txtfile\"):\n",
    "    #        os.makedirs(DIR+\"/\"+element+\"/\"+\"txtfile\")\n",
    "    #    command = \"gunzip \"+f_gz_dir+' '+DIR+\"/\"+element+\"/txtfile/\"\n",
    "    #    print(command)\n",
    "    #    os.system(command)\n",
    "    df_proj = pd.DataFrame(columns =[\"message_id\", \"thread_id\", \"author_aliase_id\", \"author_name\", \"receivers_name\", \"message_text\", \"timestamp\", \"subject\", \"reply_to\" ])\n",
    "    txtlist = os.listdir(DIR+\"/\"+element)\n",
    "\n",
    "    for fname in txtlist:\n",
    "        if '.gz' in fname:\n",
    "            txtname = un_gz(DIR+\"/\"+element+\"/\"+fname)\n",
    "            \n",
    "    txtlist = os.listdir(DIR+\"/\"+element)\n",
    "    for file_month in txtlist:\n",
    "        \n",
    "        if not ('.txt' in file_month):\n",
    "            continue\n",
    "\n",
    "        if '.gz' in file_month:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        #print(file_month)\n",
    "        messages = getMonthContent(DIR+\"/\"+element+'/'+file_month)\n",
    "        #print(len(messages))\n",
    "        element_messages.extend(messages)\n",
    "       \n",
    "        try:\n",
    "            conn.commit()\n",
    "            csv_path = DIR_csv+\"/\"+element+'/'\n",
    "            if not os.path.exists(csv_path):\n",
    "                os.makedirs(csv_path)\n",
    "            csv_file = csv_path+file_month.replace(\".txt\",'')+\".csv\"\n",
    "            #print(csv_file)\n",
    "            df_thismonth = saveMessagetocsv(messages, csv_file, element)\n",
    "        except BaseException as e:\n",
    "            print(e)\n",
    "\n",
    "        df_proj = df_proj.append(df_thismonth, ignore_index = True).drop_duplicates().dropna(how ='all', axis=0)\n",
    "    \n",
    "    checkAliase(conn, element_messages)\n",
    "    add_thread(df_proj, pjname)\n",
    "\n",
    "        \n",
    "    csv_file = csv_path+element+\"_all.csv\"\n",
    "    df_all = df_all.append(df_proj, ignore_index = True)\n",
    "    df_proj.to_csv(csv_file)\n",
    "    # Drop Timezone!!!\n",
    "    df_proj[\"timestamp\"] = df_proj[\"timestamp\"].apply(lambda x: pd.Timestamp(x).tz_localize(None)).fillna(np.nan)\n",
    "    #df_proj[\"timestamp\"] = df_proj[\"timestamp\"].apply(lambda x: '' if pd.isnull(x) else x)\n",
    "\n",
    "    try:\n",
    "    #    pass\n",
    "        # SAVE TO psql HERE!!!!!!!!!!!!\n",
    "        dataframe_to_psql(df_proj)\n",
    "    except BaseException as err:\n",
    "        print(err)\n",
    "        print(pjname)\n",
    "        break\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "df_all.to_csv('emails_all.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = pd.read_sql(\"SELECT * FROM message\", con=psql_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj = df_proj.apply(lambda x: \"NULL\" if x is np.nan else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_psql = df_proj.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj[\"timestamp\"] = df_proj[\"timestamp\"].apply(lambda x: pd.Timestamp(x).tz_localize(None)).fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_to_psql(df_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj[\"timestamp\"].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe_to_psql(df_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_proj.sort_values(by=\"message_id\", ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj.iloc[1044]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj.iloc[1045]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist_in[11:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_format = \"%Y-%B\"\n",
    "torigin = \"2007-January\"\n",
    "\n",
    "t_time = time.strptime(torigin, time_format)\n",
    "\n",
    "#pd.datetime(ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "print(pd.Timestamp(time.strftime(time_format, t_time)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_csv_route = \"./raw/openlayers-dev/2009-January.txt\"\n",
    "file_month = file_csv_route.split('/')[-1].split('.')[0]\n",
    "time_format = \"%Y-%B\"\n",
    "str_time = file_month.split('.')[0]\n",
    "time_struct = time.strptime(str_time, time_format)\n",
    "timeStamp = pd.Timestamp(file_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeStamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in element_messages:\n",
    "    if \"from\" in message:\n",
    "        print(email.utils.parseaddr(message[\"from\"])[0])\n",
    "        print(message[\"from\"].split('(')[0].replace(' at ','@'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = \"Wed Aug 25 11:09:10 2010\"\n",
    "time_format = \"%a %b %d %H:%M:%S %Y\"\n",
    "\n",
    "time.strptime(time_data, time_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj[\"timestamp\"] = df_proj[\"timestamp\"].apply(lambda x: pd.Timestamp(x).tz_localize(None)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a673315fe2889f1938749863cb31942b8222368b7a28bca37fa9b828316f0ec1"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('3.6.13': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
